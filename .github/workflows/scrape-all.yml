name: Scrape Anime & Manga Data

on:
  workflow_dispatch:
    inputs:
      mode:
        description: 'Scrape mode'
        required: true
        default: 'update'
        type: choice
        options:
          - full
          - update
      media:
        description: 'Media type to scrape'
        required: true
        default: 'both'
        type: choice
        options:
          - both
          - anime
          - manga
  schedule:
    - cron: '0 2 * * *'  # Daily at 2 AM UTC

jobs:
  # ============================================================================
  # ANIME SCRAPING JOBS (11 services)
  # ============================================================================
  
  scrape-anidb:
    if: github.event.inputs.media != 'manga' || github.event_name == 'schedule'
    runs-on: ubuntu-latest
    timeout-minutes: 30
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      - run: pip install -r requirements.txt
      - uses: actions/download-artifact@v4
        with:
          name: checkpoint-anidb-anime
          path: checkpoints/anime/
        continue-on-error: true
      - run: python scripts/run_anime_scraper.py --service anidb
      - uses: actions/upload-artifact@v4
        with:
          name: data-anidb-anime
          path: scraped-data/anime/anidb-anime.json
          retention-days: 7
      - uses: actions/upload-artifact@v4
        with:
          name: checkpoint-anidb-anime
          path: checkpoints/anime/anidb-checkpoint.json
          retention-days: 90

  scrape-anilist-anime:
    if: github.event.inputs.media != 'manga' || github.event_name == 'schedule'
    runs-on: ubuntu-latest
    timeout-minutes: 120
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      - run: pip install -r requirements.txt
      - uses: actions/download-artifact@v4
        with:
          name: checkpoint-anilist-anime
          path: checkpoints/anime/
        continue-on-error: true
      - run: python scripts/run_anime_scraper.py --service anilist
      - uses: actions/upload-artifact@v4
        with:
          name: data-anilist-anime
          path: scraped-data/anime/anilist-anime.json
          retention-days: 7
      - uses: actions/upload-artifact@v4
        with:
          name: checkpoint-anilist-anime
          path: checkpoints/anime/anilist-checkpoint.json
          retention-days: 90

  scrape-mal-anime:
    if: github.event.inputs.media != 'manga' || github.event_name == 'schedule'
    runs-on: ubuntu-latest
    timeout-minutes: 240
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      - run: pip install -r requirements.txt
      - uses: actions/download-artifact@v4
        with:
          name: checkpoint-mal-anime
          path: checkpoints/anime/
        continue-on-error: true
      - run: python scripts/run_anime_scraper.py --service myanimelist
      - uses: actions/upload-artifact@v4
        with:
          name: data-mal-anime
          path: scraped-data/anime/myanimelist-anime.json
          retention-days: 7
      - uses: actions/upload-artifact@v4
        with:
          name: checkpoint-mal-anime
          path: checkpoints/anime/myanimelist-checkpoint.json
          retention-days: 90

  scrape-kitsu-anime:
    if: github.event.inputs.media != 'manga' || github.event_name == 'schedule'
    runs-on: ubuntu-latest
    timeout-minutes: 120
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      - run: pip install -r requirements.txt
      - uses: actions/download-artifact@v4
        with:
          name: checkpoint-kitsu-anime
          path: checkpoints/anime/
        continue-on-error: true
      - name: Scrape Kitsu
        env:
          KITSU_EMAIL: ${{ secrets.KITSU_EMAIL }}
          KITSU_PASSWORD: ${{ secrets.KITSU_PASSWORD }}
        run: python scripts/run_anime_scraper.py --service kitsu
      - uses: actions/upload-artifact@v4
        with:
          name: data-kitsu-anime
          path: scraped-data/anime/kitsu-anime.json
          retention-days: 7
      - uses: actions/upload-artifact@v4
        with:
          name: checkpoint-kitsu-anime
          path: checkpoints/anime/kitsu-checkpoint.json
          retention-days: 90

  scrape-simkl:
    if: github.event.inputs.media != 'manga' || github.event_name == 'schedule'
    runs-on: ubuntu-latest
    timeout-minutes: 240
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      - run: pip install -r requirements.txt
      - uses: actions/download-artifact@v4
        with:
          name: checkpoint-simkl-anime
          path: checkpoints/anime/
        continue-on-error: true
      - name: Scrape SIMKL
        env:
          SIMKL_CLIENT_ID: ${{ secrets.SIMKL_CLIENT_ID }}
        run: python scripts/run_anime_scraper.py --service simkl
      - uses: actions/upload-artifact@v4
        with:
          name: data-simkl-anime
          path: scraped-data/anime/simkl-anime.json
          retention-days: 7
      - uses: actions/upload-artifact@v4
        with:
          name: checkpoint-simkl-anime
          path: checkpoints/anime/simkl-checkpoint.json
          retention-days: 90

  scrape-ann:
    if: github.event.inputs.media != 'manga' || github.event_name == 'schedule'
    runs-on: ubuntu-latest
    timeout-minutes: 360
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      - run: pip install -r requirements.txt
      - uses: actions/download-artifact@v4
        with:
          name: checkpoint-ann-anime
          path: checkpoints/anime/
        continue-on-error: true
      - run: python scripts/run_anime_scraper.py --service animenewsnetwork
      - uses: actions/upload-artifact@v4
        with:
          name: data-ann-anime
          path: scraped-data/anime/animenewsnetwork-anime.json
          retention-days: 7
      - uses: actions/upload-artifact@v4
        with:
          name: checkpoint-ann-anime
          path: checkpoints/anime/animenewsnetwork-checkpoint.json
          retention-days: 90

  scrape-animeplanet:
    if: github.event.inputs.media != 'manga' || github.event_name == 'schedule'
    runs-on: ubuntu-latest
    timeout-minutes: 180
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      - run: pip install -r requirements.txt
      - uses: actions/download-artifact@v4
        with:
          name: checkpoint-animeplanet-anime
          path: checkpoints/anime/
        continue-on-error: true
      - run: python scripts/run_anime_scraper.py --service animeplanet
      - uses: actions/upload-artifact@v4
        with:
          name: data-animeplanet-anime
          path: scraped-data/anime/animeplanet-anime.json
          retention-days: 7
      - uses: actions/upload-artifact@v4
        with:
          name: checkpoint-animeplanet-anime
          path: checkpoints/anime/animeplanet-checkpoint.json
          retention-days: 90

  scrape-livechart:
    if: github.event.inputs.media != 'manga' || github.event_name == 'schedule'
    runs-on: ubuntu-latest
    timeout-minutes: 120
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      - run: pip install -r requirements.txt
      - uses: actions/download-artifact@v4
        with:
          name: checkpoint-livechart-anime
          path: checkpoints/anime/
        continue-on-error: true
      - run: python scripts/run_anime_scraper.py --service livechart
      - uses: actions/upload-artifact@v4
        with:
          name: data-livechart-anime
          path: scraped-data/anime/livechart-anime.json
          retention-days: 7
      - uses: actions/upload-artifact@v4
        with:
          name: checkpoint-livechart-anime
          path: checkpoints/anime/livechart-checkpoint.json
          retention-days: 90

  scrape-tmdb:
    if: github.event.inputs.media != 'manga' || github.event_name == 'schedule'
    runs-on: ubuntu-latest
    timeout-minutes: 180
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      - run: pip install -r requirements.txt
      - uses: actions/download-artifact@v4
        with:
          name: checkpoint-tmdb-anime
          path: checkpoints/anime/
        continue-on-error: true
      - name: Scrape TMDB
        env:
          TMDB_API_KEY: ${{ secrets.TMDB_API_KEY }}
        run: python scripts/run_anime_scraper.py --service themoviedb
      - uses: actions/upload-artifact@v4
        with:
          name: data-tmdb-anime
          path: scraped-data/anime/themoviedb-anime.json
          retention-days: 7
      - uses: actions/upload-artifact@v4
        with:
          name: checkpoint-tmdb-anime
          path: checkpoints/anime/themoviedb-checkpoint.json
          retention-days: 90

  scrape-tvdb:
    if: github.event.inputs.media != 'manga' || github.event_name == 'schedule'
    runs-on: ubuntu-latest
    timeout-minutes: 180
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      - run: pip install -r requirements.txt
      - uses: actions/download-artifact@v4
        with:
          name: checkpoint-tvdb-anime
          path: checkpoints/anime/
        continue-on-error: true
      - name: Scrape TVDB
        env:
          TVDB_API_KEY: ${{ secrets.TVDB_API_KEY }}
        run: python scripts/run_anime_scraper.py --service tvdb
      - uses: actions/upload-artifact@v4
        with:
          name: data-tvdb-anime
          path: scraped-data/anime/tvdb-anime.json
          retention-days: 7
      - uses: actions/upload-artifact@v4
        with:
          name: checkpoint-tvdb-anime
          path: checkpoints/anime/tvdb-checkpoint.json
          retention-days: 90

  scrape-imdb:
    if: github.event.inputs.media != 'manga' || github.event_name == 'schedule'
    runs-on: ubuntu-latest
    timeout-minutes: 60
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      - run: pip install -r requirements.txt
      - run: python scripts/run_anime_scraper.py --service imdb
      - uses: actions/upload-artifact@v4
        with:
          name: data-imdb-anime
          path: scraped-data/anime/imdb-anime.json
          retention-days: 7

  # ============================================================================
  # MANGA SCRAPING JOBS (3 services)
  # ============================================================================

  scrape-anilist-manga:
    if: github.event.inputs.media != 'anime' || github.event_name == 'schedule'
    runs-on: ubuntu-latest
    timeout-minutes: 120
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      - run: pip install -r requirements.txt
      - uses: actions/download-artifact@v4
        with:
          name: checkpoint-anilist-manga
          path: checkpoints/manga/
        continue-on-error: true
      - run: python scripts/run_manga_scraper.py --service anilist
      - uses: actions/upload-artifact@v4
        with:
          name: data-anilist-manga
          path: scraped-data/manga/anilist-manga.json
          retention-days: 7
      - uses: actions/upload-artifact@v4
        with:
          name: checkpoint-anilist-manga
          path: checkpoints/manga/anilist-checkpoint.json
          retention-days: 90

  scrape-mal-manga:
    if: github.event.inputs.media != 'anime' || github.event_name == 'schedule'
    runs-on: ubuntu-latest
    timeout-minutes: 240
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      - run: pip install -r requirements.txt
      - uses: actions/download-artifact@v4
        with:
          name: checkpoint-mal-manga
          path: checkpoints/manga/
        continue-on-error: true
      - run: python scripts/run_manga_scraper.py --service myanimelist
      - uses: actions/upload-artifact@v4
        with:
          name: data-mal-manga
          path: scraped-data/manga/myanimelist-manga.json
          retention-days: 7
      - uses: actions/upload-artifact@v4
        with:
          name: checkpoint-mal-manga
          path: checkpoints/manga/myanimelist-checkpoint.json
          retention-days: 90

  scrape-kitsu-manga:
    if: github.event.inputs.media != 'anime' || github.event_name == 'schedule'
    runs-on: ubuntu-latest
    timeout-minutes: 120
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      - run: pip install -r requirements.txt
      - uses: actions/download-artifact@v4
        with:
          name: checkpoint-kitsu-manga
          path: checkpoints/manga/
        continue-on-error: true
      - name: Scrape Kitsu
        env:
          KITSU_EMAIL: ${{ secrets.KITSU_EMAIL }}
          KITSU_PASSWORD: ${{ secrets.KITSU_PASSWORD }}
        run: python scripts/run_manga_scraper.py --service kitsu
      - uses: actions/upload-artifact@v4
        with:
          name: data-kitsu-manga
          path: scraped-data/manga/kitsu-manga.json
          retention-days: 7
      - uses: actions/upload-artifact@v4
        with:
          name: checkpoint-kitsu-manga
          path: checkpoints/manga/kitsu-checkpoint.json
          retention-days: 90

  # ============================================================================
  # ANIME MAPPING JOB
  # ============================================================================
 
  map-anime:
    if: github.event.inputs.media != 'manga' || github.event_name == 'schedule'
    needs: [scrape-anidb, scrape-anilist-anime, scrape-mal-anime, scrape-kitsu-anime, scrape-simkl, scrape-ann, scrape-animeplanet, scrape-livechart, scrape-tmdb, scrape-tvdb, scrape-imdb]
    runs-on: ubuntu-latest
    timeout-minutes: 30
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      - run: pip install -r requirements.txt
       
      - name: Download all anime artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: data-*-anime
          path: artifacts/
       
      - name: Move artifacts to scraped-data
        run: |
          mkdir -p scraped-data/anime
          find artifacts -name "*.json" -exec cp {} scraped-data/anime/ \;
          ls -la scraped-data/anime/
       
      - name: Run anime mapper
        run: python scripts/run_mapper.py --type anime
       
      - name: Compress large JSON files (>50MB)
        run: |
          cd scraped-data/anime
          for file in *.json; do
            SIZE=$(stat -f%z "$file" 2>/dev/null || stat -c%s "$file" 2>/dev/null)
            if [ $SIZE -gt 52428800 ]; then
              echo "Compressing $file ($(($SIZE / 1048576))MB)..."
              # ADD THE -f FLAG HERE
              gzip -9f "$file"
              echo "  → ${file}.gz"
            fi
          done
          cd ../..
          ls -lh scraped-data/anime/
       
      - name: Commit all anime data (service files + mapped)
        run: |
          git config --local user.email "github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"
            
          # Add all anime scraped data (compressed and regular)
          git add scraped-data/anime/*.json scraped-data/anime/*.json.gz
            
          # Add final mapped file
          git add mapped-data/anime-list-full-mapped.json
            
          # Commit if there are changes, then pull --rebase, then push
          git diff --quiet && git diff --staged --quiet || \
            (git commit -m "Update anime data: $(date +'%Y-%m-%d %H:%M:%S')" && \
             git pull --rebase origin main && \
             git push)
       
      - name: Anime Summary
        if: always()
        run: |
          echo "# Anime Scraping Complete" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Date:** $(date)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
           
          echo "## Service Files Scraped" >> $GITHUB_STEP_SUMMARY
          for file in scraped-data/anime/*; do
            if [ -f "$file" ]; then
              SIZE=$(du -h "$file" | cut -f1)
              NAME=$(basename "$file")
              if [[ "$NAME" == *.json ]]; then
                COUNT=$(python3 -c "import json; print(len(json.load(open('$file'))))" 2>/dev/null || echo "0")
                echo "- $NAME: $COUNT items ($SIZE)" >> $GITHUB_STEP_SUMMARY
              else
                echo "- $NAME: compressed ($SIZE)" >> $GITHUB_STEP_SUMMARY
              fi
            fi
          done
           
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## Final Mapped File" >> $GITHUB_STEP_SUMMARY
          if [ -f "mapped-data/anime-list-full-mapped.json" ]; then
            TOTAL=$(python3 -c "import json; print(len(json.load(open('mapped-data/anime-list-full-mapped.json'))))")
            SIZE=$(du -h mapped-data/anime-list-full-mapped.json | cut -f1)
            echo "- Total unique anime: $TOTAL" >> $GITHUB_STEP_SUMMARY
            echo "- File size: $SIZE" >> $GITHUB_STEP_SUMMARY
          fi

  # ============================================================================
  # MANGA MAPPING JOB
  # ============================================================================
  
  map-manga:
    if: github.event.inputs.media != 'anime' || github.event_name == 'schedule'
    needs: [scrape-anilist-manga, scrape-mal-manga, scrape-kitsu-manga]
    runs-on: ubuntu-latest
    timeout-minutes: 30
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      - run: pip install -r requirements.txt
      
      - name: Download all manga artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: data-*-manga
          path: artifacts/
      
      - name: Move artifacts to scraped-data
        run: |
          mkdir -p scraped-data/manga
          find artifacts -name "*.json" -exec cp {} scraped-data/manga/ \;
          ls -la scraped-data/manga/
      
      - name: Run manga mapper
        run: python scripts/run_mapper.py --type manga
      
      - name: Compress large JSON files (>50MB)
        run: |
          cd scraped-data/manga
          for file in *.json; do
            SIZE=$(stat -f%z "$file" 2>/dev/null || stat -c%s "$file" 2>/dev/null)
            if [ $SIZE -gt 52428800 ]; then
              echo "Compressing $file ($(($SIZE / 1048576))MB)..."
              # ADD THE -f FLAG HERE
              gzip -9f "$file"
              echo "  → ${file}.gz"
            fi
          done
          cd ../..
          ls -lh scraped-data/manga/
      
      - name: Commit manga data (compressed large files)
        run: |
          git config --local user.email "github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"
          
          # Add compressed and regular files
          git add scraped-data/manga/*.json scraped-data/manga/*.json.gz
          git add mapped-data/manga-list-full-mapped.json
          
          # Commit if there are changes, then pull --rebase, then push
          git diff --quiet && git diff --staged --quiet || \
            (git commit -m "Update manga data: $(date +'%Y-%m-%d %H:%M:%S')" && \
             git pull --rebase origin main && \
             git push)
      
      - name: Manga Summary
        if: always()
        run: |
          echo "# Manga Scraping Complete" >> $GITHUB_STEP_SUMMARY
          echo "**Date:** $(date)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## Service Files" >> $GITHUB_STEP_SUMMARY
          for file in scraped-data/manga/*; do
            if [ -f "$file" ]; then
              SIZE=$(du -h "$file" | cut -f1)
              NAME=$(basename "$file")
              if [[ "$NAME" == *.json ]]; then
                COUNT=$(python3 -c "import json; print(len(json.load(open('$file'))))" 2>/dev/null || echo "0")
                echo "- $NAME: $COUNT items ($SIZE)" >> $GITHUB_STEP_SUMMARY
              else
                echo "- $NAME: compressed ($SIZE)" >> $GITHUB_STEP_SUMMARY
              fi
            fi
          done

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## Final Mapped File" >> $GITHUB_STEP_SUMMARY
          if [ -f "mapped-data/manga-list-full-mapped.json" ]; then
            TOTAL=$(python3 -c "import json; print(len(json.load(open('mapped-data/manga-list-full-mapped.json'))))")
            SIZE=$(du -h mapped-data/manga-list-full-mapped.json | cut -f1)
            echo "- Total unique manga: $TOTAL" >> $GITHUB_STEP_SUMMARY
            echo "- File size: $SIZE" >> $GITHUB_STEP_SUMMARY
          fi
